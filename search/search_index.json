{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLflow Sweep","text":"MLflow Sweep: A tool for hyperparameter optimization with MLflow <p>MLflow sweep is a simple extension to the MLflow framework for running hyperparameter sweeps. In particular, it adds the following functionality compared to running MLflow experiments directly:</p> <ul> <li> <p>Sweep configuration: Define a sweep configuration file that specifies the command to run, the parameters to     sweep over, and the sweep strategy. Support for multiple sweep strategies (grid search, random) and different     parameter distributions (categorical, uniform, normal, log uniform).</p> </li> <li> <p>No code change: No changes to your code are required to run sweeps. You can use the same code you would use for     running a single experiment.</p> </li> <li> <p>Auto grouping: Automatically groups runs by sweep name, making it easy to track and compare results.</p> </li> <li> <p>Visualization: Provides custom visualizations for sweep results, including parameter importance and performance     metrics.</p> </li> </ul> <p>The package was heavily inspired by the sweep functionality of Weights &amp; Biases and aims to provide a similar experience for MLflow users. It is designed to be easy to use and integrate with existing MLflow workflows.</p>"},{"location":"#installation","title":"\ud83d\udd27 Installation","text":"pipuv <pre><code>pip install mlflow-sweep\n</code></pre> <pre><code>uv add mlflow-sweep\n</code></pre> <p>And that's it. You should now be able to see an additional <code>sweep</code> command in the MLflow CLI.</p> <pre><code>\u276f mlflow --help\nUsage: mlflow [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  artifacts    Upload, list, and download artifacts from an MLflow...\n  db           Commands for managing an MLflow tracking database.\n  deployments  Deploy MLflow models to custom targets.\n  doctor       Prints out useful information for debugging issues with MLflow.\n  experiments  Manage experiments.\n  gc           Permanently delete runs in the `deleted` lifecycle stage.\n  models       Deploy MLflow models locally.\n  run          Run an MLflow project from the given URI.\n  runs         Manage runs.\n  sagemaker    Serve models on SageMaker.\n  server       Run the MLflow tracking server.\n  sweep        MLflow Sweep CLI commands.            &lt;-- this is the new command\n</code></pre> <p>To learn more about the <code>sweep</code> command and how to configure and start a run, visit the following sections of the documentation.</p> <ul> <li> <p> Quickstart guide on how to use MLflow sweep</p> <p>A simple guide to get you started with MLflow sweep, including how to create a sweep configuration file and run sweeps</p> <p> Getting started</p> </li> <li> <p> Configuration</p> <p>A detailed description of the configuration file format, including how to specify commands, parameters, and sweep strategies. This is the core object that needs to be created to use this package.</p> <p> Configuration</p> </li> <li> <p> Examples</p> <p>A collection of examples demonstrating how to use MLflow sweep with different machine learning frameworks and scenarios. This is a great way to see how the package can be used in practice and to get inspiration for your own sweeps.</p> <p> Examples</p> </li> <li> <p> API references</p> <p>A detailed outline of modules, classes, and functions in the package. This is useful for developers who want to understand the internals of the package or extend it with custom functionality.</p> <p> API references</p> </li> </ul>"},{"location":"#development-setup","title":"\ud83e\uddd1\u200d\ud83d\udcbb Development setup","text":"<p>Mlflow sweep uses uv for development and packaging. To set up the development environment run the following commands:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/mlflow_sweep\ncd mlflow_sweep\n\n# Install using uv with development dependencies\nuv sync --dev\n</code></pre> <p>py-invoke is used for running common tasks, such as running tests, building the package, and generating documentation.</p> <pre><code>\u276f uv run invoke --list\nAvailable tasks:\n\n    all        Run all tasks.\n    check      Check code with pre-commit.\n    clean      Clean up build artifacts.\n    docs       Build the documentation.\n    doctests   Run doctests.\n    tests      Test and coverage.\n</code></pre>"},{"location":"#license","title":"\u2755 License","text":"<p>Package is licensed under Apache 2.0 license. See the LICENSE file for details. If you use this tool in your research, please cite it as:</p> <pre><code>@misc{mlflow_sweep,\n    author       = {Nicki Skafte Detlefsen},\n    title        = {MLflow Sweep: A tool for hyperparameter optimization with MLflow},\n    howpublished = {\\url{https://github.com/SkafteNicki/mlflow_sweep}},\n    year         = {2025}\n}\n</code></pre>"},{"location":"api_references/","title":"\ud83d\udcd8 API reference","text":""},{"location":"api_references/#_1","title":"\ud83d\udcd8 API reference","text":"<p>Base API Documentation. The content of this file is auto-generated from the source code.</p>"},{"location":"api_references/#mlflow_sweep.commands","title":"<code>mlflow_sweep.commands</code>","text":""},{"location":"api_references/#mlflow_sweep.commands.determine_sweep","title":"<code>determine_sweep(sweep_id)</code>","text":"<p>Determine the sweep to use. If a sweep_id is provided, it will be used. Otherwise, the most recent sweep will be selected.</p> Source code in <code>src/mlflow_sweep/commands.py</code> <pre><code>def determine_sweep(sweep_id: str) -&gt; Run:\n    \"\"\"Determine the sweep to use.\n    If a sweep_id is provided, it will be used. Otherwise, the most recent sweep will be selected.\"\"\"\n\n    sweeps: list[Run] = mlflow.search_runs(  # ty: ignore[invalid-assignment]\n        search_all_experiments=True,\n        filter_string=\"tag.sweep = 'True'\",\n        output_format=\"list\",\n    )\n\n    if sweep_id:\n        for sweep in sweeps:\n            if sweep.info.run_id == sweep_id:\n                break\n        else:\n            raise ValueError(f\"No sweep found with sweep_id: {sweep_id}\")\n    else:\n        sweep = max(sweeps, key=lambda x: x.info.start_time)  # Get the most recent sweep\n\n    return sweep\n</code></pre>"},{"location":"api_references/#mlflow_sweep.commands.finalize_command","title":"<code>finalize_command(sweep_id='')</code>","text":"<p>Finalize a sweep.</p> Source code in <code>src/mlflow_sweep/commands.py</code> <pre><code>def finalize_command(sweep_id: str = \"\") -&gt; None:\n    \"\"\"Finalize a sweep.\"\"\"\n    sweep = determine_sweep(sweep_id)\n    config = SweepConfig.from_sweep(sweep)\n    runstate = SweepState(sweep_id=sweep.info.run_id)\n    all_runs = runstate.get_all()\n\n    mlflow.set_experiment(experiment_id=sweep.info.experiment_id)\n    mlflow.start_run(run_id=sweep.info.run_id)\n\n    data = pd.DataFrame(\n        {\n            \"start\": [current_time_convert(run.start_time) for run in all_runs],\n            \"end\": [current_time_convert(run.end_time) for run in all_runs],\n            \"run\": [run.id for run in all_runs],\n            \"status\": [run.state for run in all_runs],\n        }\n    )\n    data.sort_values(by=\"start\", inplace=True)\n    fig = plot_trial_timeline(df=data)\n    fig.write_html(\"run_timeline.html\")\n    mlflow.log_artifact(\"run_timeline.html\")\n    Path(\"run_timeline.html\").unlink(missing_ok=True)\n\n    if config.metric is not None:\n        metric_values = np.array([run.summary_metrics.get(config.metric.name) for run in all_runs])\n        parameter_values = {\n            param_name: np.array([run.config[param_name][\"value\"] for run in all_runs])\n            for param_name in config.parameters\n        }\n\n        features = calculate_feature_importance_and_correlation(metric_values, parameter_values)\n\n        # Create the table\n        table = Table(title=f\"Feature Importance and Correlation for {config.metric.name}\", show_lines=True)\n\n        # Add columns\n        table.add_column(\"Parameter\", style=\"bold magenta\")\n        table.add_column(\"Importance\", justify=\"right\")\n        table.add_column(\"Permutation Importance\", justify=\"right\")\n        table.add_column(\"Pearson\", justify=\"right\")\n        table.add_column(\"Spearman\", justify=\"right\")\n\n        # Add rows\n        for param, stats in features.items():\n            table.add_row(\n                param,\n                f\"{stats['importance']:.4f}\",\n                f\"{stats['permutation_importance']:.4f}\",\n                f\"{stats['pearson']:.4f}\",\n                f\"{stats['spearman']:.4f}\",\n            )\n\n        # Print using rich console\n        console = Console()\n        console.print(table)\n\n        data = pd.DataFrame(\n            {\n                \"created\": [current_time_convert(run.start_time) for run in all_runs],\n                config.metric.name: [run.summary_metrics.get(config.metric.name) for run in all_runs],\n            }\n        )\n\n        fig = plot_metric_vs_time(data, time_col=\"created\", metric_col=config.metric.name)\n        fig.write_html(\"metric_vs_time.html\")\n        mlflow.log_artifact(\"metric_vs_time.html\")\n        Path(\"metric_vs_time.html\").unlink(missing_ok=True)\n\n        fig = plot_parameter_importance_and_correlation(features, metric_name=config.metric.name)\n        fig.write_html(\"parameter_importance_and_correlation.html\")\n        mlflow.log_artifact(\"parameter_importance_and_correlation.html\")\n        Path(\"parameter_importance_and_correlation.html\").unlink(missing_ok=True)\n</code></pre>"},{"location":"api_references/#mlflow_sweep.commands.init_command","title":"<code>init_command(config_path)</code>","text":"<p>Start a sweep from a config.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Path</code> <p>Path to the sweep configuration file.</p> required Source code in <code>src/mlflow_sweep/commands.py</code> <pre><code>def init_command(config_path: Path) -&gt; None:\n    \"\"\"Start a sweep from a config.\n\n    Args:\n        config_path (Path): Path to the sweep configuration file.\n\n    \"\"\"\n    with Path(config_path).open() as file:\n        config = yaml.safe_load(file)\n\n    config = SweepConfig(**config)  # validate the config\n    rprint(\"[bold blue]Initializing sweep with configuration:[/bold blue]\")\n    rprint(config)\n\n    mlflow.set_experiment(config.experiment_name)\n    run = mlflow.start_run(run_name=config.sweep_name)\n    mlflow.set_tag(\"sweep\", True)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmp_config_path = Path(tmpdir) / \"sweep_config.yaml\"\n        shutil.copy(config_path, tmp_config_path)\n        mlflow.log_artifact(str(tmp_config_path))\n\n    rprint(f\"[bold green]Sweep initialized with ID: {run.info.run_id}[/bold green]\")\n</code></pre>"},{"location":"api_references/#mlflow_sweep.commands.run_command","title":"<code>run_command(sweep_id='')</code>","text":"<p>Run a sweep agent.</p> Source code in <code>src/mlflow_sweep/commands.py</code> <pre><code>def run_command(sweep_id: str = \"\") -&gt; None:\n    \"\"\"Run a sweep agent.\"\"\"\n    sweep = determine_sweep(sweep_id)\n\n    config = SweepConfig.from_sweep(sweep)\n    runstate = SweepState(sweep_id=sweep.info.run_id)\n    sweep_sampler = SweepSampler(config, runstate)\n\n    mlflow.set_experiment(experiment_id=sweep.info.experiment_id)\n    mlflow.start_run(run_id=sweep.info.run_id)\n\n    # Set an environment variable to link runs in the sweep\n    # This will be picked up by the custom SweepRunContextProvider\n    global_env = os.environ.copy()\n    global_env[\"SWEEP_PARENT_RUN_ID\"] = sweep.info.run_id\n    global_env[\"SWEEP_AGENT_ID\"] = str(uuid.uuid4())  # Unique ID for this agent\n\n    while True:\n        output = sweep_sampler.propose_next()\n        if output is None:\n            rprint(\"[bold red]No more runs can be proposed or run cap reached.[/bold red]\")\n            break\n        command, data = output\n        mlflow.log_table(\n            data={k: [v] for k, v in data.items()},\n            artifact_file=\"proposed_parameters.json\",\n        )\n        rprint(f\"[bold blue]Executed command:[/bold blue] \\n[italic]{command}[/italic]\")\n        rprint(50 * \"\u2500\")\n        local_env = global_env.copy()\n        local_env[\"SWEEP_RUN_ID\"] = data[\"sweep_run_id\"]\n        subprocess.run(command, shell=True, env=local_env, check=True)\n        rprint(50 * \"\u2500\")\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models","title":"<code>mlflow_sweep.models</code>","text":""},{"location":"api_references/#mlflow_sweep.models.ExtendedSweepRun","title":"<code>ExtendedSweepRun</code>","text":"<p>               Bases: <code>SweepRun</code></p> <p>Extended SweepRun to include additional information.</p> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class ExtendedSweepRun(SweepRun):\n    \"\"\"Extended SweepRun to include additional information.\"\"\"\n\n    id: str\n    start_time: int\n    end_time: int\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.GoalEnum","title":"<code>GoalEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for sweep goals.</p> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class GoalEnum(str, Enum):\n    \"\"\"Enumeration for sweep goals.\"\"\"\n\n    maximize = \"maximize\"\n    minimize = \"minimize\"\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.MetricConfig","title":"<code>MetricConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the metric to track during the sweep.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the metric to track.</p> <code>goal</code> <code>GoalEnum</code> <p>Goal for the metric, either 'maximize' or 'minimize'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metric = MetricConfig(name=\"accuracy\", goal=\"maximize\")\n&gt;&gt;&gt; metric.name\n'accuracy'\n&gt;&gt;&gt; metric.goal\n&lt;GoalEnum.maximize: 'maximize'&gt;\n&gt;&gt;&gt; metric_min = MetricConfig(name=\"error_rate\", goal=\"minimize\")\n&gt;&gt;&gt; metric_min.goal\n&lt;GoalEnum.minimize: 'minimize'&gt;\n</code></pre> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class MetricConfig(BaseModel):\n    \"\"\"Configuration for the metric to track during the sweep.\n\n    Attributes:\n        name (str): Name of the metric to track.\n        goal (GoalEnum): Goal for the metric, either 'maximize' or 'minimize'.\n\n    Examples:\n        &gt;&gt;&gt; metric = MetricConfig(name=\"accuracy\", goal=\"maximize\")\n        &gt;&gt;&gt; metric.name\n        'accuracy'\n        &gt;&gt;&gt; metric.goal\n        &lt;GoalEnum.maximize: 'maximize'&gt;\n        &gt;&gt;&gt; metric_min = MetricConfig(name=\"error_rate\", goal=\"minimize\")\n        &gt;&gt;&gt; metric_min.goal\n        &lt;GoalEnum.minimize: 'minimize'&gt;\n    \"\"\"\n\n    name: str = Field(..., description=\"Name of the metric to track\")\n    goal: GoalEnum = Field(..., description=\"Goal for the metric (e.g., 'maximize', 'minimize')\")\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.MetricHistory","title":"<code>MetricHistory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stores metric history for a specific run.</p> <p>Attributes:</p> Name Type Description <code>run_id</code> <code>str</code> <p>Run ID associated with the metric history.</p> <code>metrics</code> <code>list[dict]</code> <p>List of metric dictionaries for the run.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; metrics = [{\"key\": \"accuracy\", \"value\": 0.95, \"step\": 0, \"timestamp\": 1234567890}]\n&gt;&gt;&gt; history = MetricHistory(run_id=\"abc123\", metrics=metrics)\n&gt;&gt;&gt; history.run_id\n'abc123'\n&gt;&gt;&gt; len(history.metrics)\n1\n&gt;&gt;&gt; history.metrics[0][\"key\"]\n'accuracy'\n&gt;&gt;&gt; history.metrics[0][\"value\"]\n0.95\n</code></pre> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class MetricHistory(BaseModel):\n    \"\"\"Stores metric history for a specific run.\n\n    Attributes:\n        run_id (str): Run ID associated with the metric history.\n        metrics (list[dict]): List of metric dictionaries for the run.\n\n    Examples:\n        &gt;&gt;&gt; metrics = [{\"key\": \"accuracy\", \"value\": 0.95, \"step\": 0, \"timestamp\": 1234567890}]\n        &gt;&gt;&gt; history = MetricHistory(run_id=\"abc123\", metrics=metrics)\n        &gt;&gt;&gt; history.run_id\n        'abc123'\n        &gt;&gt;&gt; len(history.metrics)\n        1\n        &gt;&gt;&gt; history.metrics[0][\"key\"]\n        'accuracy'\n        &gt;&gt;&gt; history.metrics[0][\"value\"]\n        0.95\n    \"\"\"\n\n    run_id: str = Field(..., description=\"Run IDs associated with the metric history\")\n    metrics: list[dict] = Field(..., description=\"List of metric dicts for the run\")\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.SweepConfig","title":"<code>SweepConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a sweep in MLflow.</p> <p>Attributes:</p> Name Type Description <code>command</code> <code>str</code> <p>Command to run for each sweep trial.</p> <code>experiment_name</code> <code>str</code> <p>Name of the MLflow experiment.</p> <code>sweep_name</code> <code>str</code> <p>Name of the sweep, generated if not provided.</p> <code>method</code> <code>SweepMethodEnum</code> <p>Method for the sweep (e.g., 'grid', 'random').</p> <code>metric</code> <code>MetricConfig | None</code> <p>Configuration for the metric to track.</p> <code>parameters</code> <code>dict[str, dict]</code> <p>List of parameters to sweep over.</p> <code>run_cap</code> <code>int</code> <p>Maximum number of runs to execute in the sweep.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; params = {\"learning_rate\": {\"distribution\": \"uniform\", \"min\": 0.0001, \"max\": 0.1}}\n&gt;&gt;&gt; config = SweepConfig(\n...     command=\"python train.py\",\n...     parameters=params,\n...     metric=MetricConfig(name=\"accuracy\", goal=\"maximize\"),\n...     method=\"random\"\n... )\n&gt;&gt;&gt; config.experiment_name\n'Default'\n&gt;&gt;&gt; config.run_cap\n10\n&gt;&gt;&gt; config.method\n&lt;SweepMethodEnum.random: 'random'&gt;\n&gt;&gt;&gt; config.metric.name\n'accuracy'\n</code></pre> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class SweepConfig(BaseModel):\n    \"\"\"Configuration for a sweep in MLflow.\n\n    Attributes:\n        command (str): Command to run for each sweep trial.\n        experiment_name (str): Name of the MLflow experiment.\n        sweep_name (str): Name of the sweep, generated if not provided.\n        method (SweepMethodEnum): Method for the sweep (e.g., 'grid', 'random').\n        metric (MetricConfig | None): Configuration for the metric to track.\n        parameters (dict[str, dict]): List of parameters to sweep over.\n        run_cap (int): Maximum number of runs to execute in the sweep.\n\n    Examples:\n        &gt;&gt;&gt; params = {\"learning_rate\": {\"distribution\": \"uniform\", \"min\": 0.0001, \"max\": 0.1}}\n        &gt;&gt;&gt; config = SweepConfig(\n        ...     command=\"python train.py\",\n        ...     parameters=params,\n        ...     metric=MetricConfig(name=\"accuracy\", goal=\"maximize\"),\n        ...     method=\"random\"\n        ... )\n        &gt;&gt;&gt; config.experiment_name\n        'Default'\n        &gt;&gt;&gt; config.run_cap\n        10\n        &gt;&gt;&gt; config.method\n        &lt;SweepMethodEnum.random: 'random'&gt;\n        &gt;&gt;&gt; config.metric.name\n        'accuracy'\n\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    command: str = Field(..., description=\"Command to run for each sweep trial\")\n    experiment_name: str = Field(\"Default\", description=\"Name of the MLflow experiment\")\n    sweep_name: str = Field(default_factory=lambda: \"sweep-\" + _generate_random_name(), description=\"Name of the sweep\")\n    method: SweepMethodEnum = Field(SweepMethodEnum.random, description=\"Method for the sweep (e.g., 'grid', 'random')\")\n    metric: MetricConfig | None = Field(None, description=\"Configuration for the metric to track\")\n    parameters: dict[str, dict] = Field(..., description=\"List of parameters to sweep over\")\n    run_cap: int = Field(10, description=\"Maximum number of runs to execute in the sweep\")\n\n    def model_post_init(self, context):\n        \"\"\"Validate the sweep configuration after initialization.\"\"\"\n        if self.method == SweepMethodEnum.bayes and self.metric is None:\n            raise ValueError(\"Bayesian sweeps require a metric configuration.\")\n\n    @classmethod\n    def from_sweep(cls, sweep: Run) -&gt; \"SweepConfig\":\n        \"\"\"Create a SweepConfig instance from an MLflow Run object.\n\n        This method extracts the sweep configuration from the artifacts of an MLflow Run\n        by loading the sweep_config.yaml file from the run's artifact directory.\n\n        Args:\n            sweep (Run): An MLflow Run object containing sweep configuration artifacts.\n\n        Returns:\n            SweepConfig: A validated SweepConfig instance constructed from the run's artifacts.\n\n        Raises:\n            FileNotFoundError: If the sweep_config.yaml file cannot be found in the run artifacts.\n            ValueError: If the loaded configuration is invalid or missing required fields.\n\n        Examples:\n            &gt;&gt;&gt; from mlflow.entities import Run\n            &gt;&gt;&gt; run = mlflow.get_run(\"sweep_run_id\")  # doctest: +SKIP\n            &gt;&gt;&gt; config = SweepConfig.from_sweep(run)  # doctest: +SKIP\n            &gt;&gt;&gt; config.method  # doctest: +SKIP\n            &lt;SweepMethodEnum.random: 'random'&gt;\n        \"\"\"\n        artifact_uri = sweep.info.artifact_uri.replace(\"file://\", \"\")\n        config_file_path = Path(artifact_uri) / \"sweep_config.yaml\"\n\n        with Path.open(config_file_path) as file:\n            config = yaml.safe_load(file)\n\n        return cls(**config)  # Validate the config\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.SweepConfig.from_sweep","title":"<code>from_sweep(sweep)</code>  <code>classmethod</code>","text":"<p>Create a SweepConfig instance from an MLflow Run object.</p> <p>This method extracts the sweep configuration from the artifacts of an MLflow Run by loading the sweep_config.yaml file from the run's artifact directory.</p> <p>Parameters:</p> Name Type Description Default <code>sweep</code> <code>Run</code> <p>An MLflow Run object containing sweep configuration artifacts.</p> required <p>Returns:</p> Name Type Description <code>SweepConfig</code> <code>SweepConfig</code> <p>A validated SweepConfig instance constructed from the run's artifacts.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the sweep_config.yaml file cannot be found in the run artifacts.</p> <code>ValueError</code> <p>If the loaded configuration is invalid or missing required fields.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mlflow.entities import Run\n&gt;&gt;&gt; run = mlflow.get_run(\"sweep_run_id\")\n&gt;&gt;&gt; config = SweepConfig.from_sweep(run)\n&gt;&gt;&gt; config.method\n&lt;SweepMethodEnum.random: 'random'&gt;\n</code></pre> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>@classmethod\ndef from_sweep(cls, sweep: Run) -&gt; \"SweepConfig\":\n    \"\"\"Create a SweepConfig instance from an MLflow Run object.\n\n    This method extracts the sweep configuration from the artifacts of an MLflow Run\n    by loading the sweep_config.yaml file from the run's artifact directory.\n\n    Args:\n        sweep (Run): An MLflow Run object containing sweep configuration artifacts.\n\n    Returns:\n        SweepConfig: A validated SweepConfig instance constructed from the run's artifacts.\n\n    Raises:\n        FileNotFoundError: If the sweep_config.yaml file cannot be found in the run artifacts.\n        ValueError: If the loaded configuration is invalid or missing required fields.\n\n    Examples:\n        &gt;&gt;&gt; from mlflow.entities import Run\n        &gt;&gt;&gt; run = mlflow.get_run(\"sweep_run_id\")  # doctest: +SKIP\n        &gt;&gt;&gt; config = SweepConfig.from_sweep(run)  # doctest: +SKIP\n        &gt;&gt;&gt; config.method  # doctest: +SKIP\n        &lt;SweepMethodEnum.random: 'random'&gt;\n    \"\"\"\n    artifact_uri = sweep.info.artifact_uri.replace(\"file://\", \"\")\n    config_file_path = Path(artifact_uri) / \"sweep_config.yaml\"\n\n    with Path.open(config_file_path) as file:\n        config = yaml.safe_load(file)\n\n    return cls(**config)  # Validate the config\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.SweepConfig.model_post_init","title":"<code>model_post_init(context)</code>","text":"<p>Validate the sweep configuration after initialization.</p> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>def model_post_init(self, context):\n    \"\"\"Validate the sweep configuration after initialization.\"\"\"\n    if self.method == SweepMethodEnum.bayes and self.metric is None:\n        raise ValueError(\"Bayesian sweeps require a metric configuration.\")\n</code></pre>"},{"location":"api_references/#mlflow_sweep.models.SweepMethodEnum","title":"<code>SweepMethodEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for sweep methods.</p> Source code in <code>src/mlflow_sweep/models.py</code> <pre><code>class SweepMethodEnum(str, Enum):\n    \"\"\"Enumeration for sweep methods.\"\"\"\n\n    grid = \"grid\"\n    random = \"random\"\n    bayes = \"bayes\"\n</code></pre>"},{"location":"api_references/#mlflow_sweep.plotting","title":"<code>mlflow_sweep.plotting</code>","text":""},{"location":"api_references/#mlflow_sweep.plotting.plot_metric_vs_time","title":"<code>plot_metric_vs_time(dataframe, time_col='created', metric_col='accuracy')</code>","text":"<p>Plots a metric vs. time using Plotly, with a line showing the best-so-far metric value.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>DataFrame containing the data.</p> required <code>time_col</code> <code>str</code> <p>Column name for timestamps (default is 'created').</p> <code>'created'</code> <code>metric_col</code> <code>str</code> <p>Column name for the metric being plotted (default is 'accuracy').</p> <code>'accuracy'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plotly.graph_objects.Figure: The generated interactive Plotly figure.</p> Example <p>import pandas as pd data = { ...     'created': [ ...         '2025-02-08 09:45:00', '2025-02-08 09:45:30', '2025-02-08 09:46:00', ...         '2025-02-08 09:46:30', '2025-02-08 09:47:00', '2025-02-08 09:47:30', ...         '2025-02-08 09:48:00', '2025-02-08 09:48:30', '2025-02-08 09:49:00', ...         '2025-02-08 09:49:30' ...     ], ...     'accuracy': [0.942, 0.958, 0.966, 0.958, 0.966, 0.975, 0.966, 0.958, 0.975, 0.966] ... } df = pd.DataFrame(data) fig = plot_metric_vs_time(df)</p> Source code in <code>src/mlflow_sweep/plotting.py</code> <pre><code>def plot_metric_vs_time(dataframe: pd.DataFrame, time_col: str = \"created\", metric_col: str = \"accuracy\") -&gt; go.Figure:\n    \"\"\"\n    Plots a metric vs. time using Plotly, with a line showing the best-so-far metric value.\n\n    Parameters:\n        dataframe (pd.DataFrame): DataFrame containing the data.\n        time_col (str): Column name for timestamps (default is 'created').\n        metric_col (str): Column name for the metric being plotted (default is 'accuracy').\n\n    Returns:\n        plotly.graph_objects.Figure: The generated interactive Plotly figure.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = {\n        ...     'created': [\n        ...         '2025-02-08 09:45:00', '2025-02-08 09:45:30', '2025-02-08 09:46:00',\n        ...         '2025-02-08 09:46:30', '2025-02-08 09:47:00', '2025-02-08 09:47:30',\n        ...         '2025-02-08 09:48:00', '2025-02-08 09:48:30', '2025-02-08 09:49:00',\n        ...         '2025-02-08 09:49:30'\n        ...     ],\n        ...     'accuracy': [0.942, 0.958, 0.966, 0.958, 0.966, 0.975, 0.966, 0.958, 0.975, 0.966]\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; fig = plot_metric_vs_time(df)\n    \"\"\"\n    df = dataframe.copy()\n    df[time_col] = pd.to_datetime(df[time_col])\n\n    # Calculate best-so-far metric value\n    df.sort_values(by=time_col, inplace=True)\n    df[\"best_so_far\"] = df[metric_col].cummax()\n\n    # Scatter plot of all points\n    fig = px.scatter(\n        df,\n        x=time_col,\n        y=metric_col,\n        color=metric_col,\n        title=f\"{metric_col} v. {time_col}\",\n        labels={time_col: time_col.capitalize(), metric_col: metric_col.capitalize()},\n        size=[10] * len(df),  # Fixed size for all points\n    )\n\n    # Add line for best-so-far\n    fig.add_trace(\n        go.Scatter(\n            x=df[time_col],\n            y=df[\"best_so_far\"],\n            mode=\"lines+markers\",\n            line={\"color\": \"skyblue\"},\n            name=\"Best so far\",\n            showlegend=False,\n        )\n    )\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title=time_col.capitalize(),\n        yaxis_title=metric_col.capitalize(),\n        title={\"x\": 0.5},\n        margin={\"l\": 40, \"r\": 20, \"t\": 40, \"b\": 40},\n        height=400,\n    )\n\n    return fig\n</code></pre>"},{"location":"api_references/#mlflow_sweep.plotting.plot_parameter_importance_and_correlation","title":"<code>plot_parameter_importance_and_correlation(results, metric_name='accuracy')</code>","text":"<p>Plot parameter importance and correlation with respect to a metric using Plotly. Creates a 2x2 grid with separate plots for each score type.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Output from calculate_feature_importance_and_correlation().</p> required <code>metric_name</code> <code>str</code> <p>Name of the metric (e.g., \"accuracy\", \"loss\").</p> <code>'accuracy'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plotly.graph_objects.Figure: Interactive 2x2 grid figure of bar plots.</p> Source code in <code>src/mlflow_sweep/plotting.py</code> <pre><code>def plot_parameter_importance_and_correlation(results: dict, metric_name: str = \"accuracy\") -&gt; go.Figure:\n    \"\"\"\n    Plot parameter importance and correlation with respect to a metric using Plotly.\n    Creates a 2x2 grid with separate plots for each score type.\n\n    Args:\n        results (dict): Output from calculate_feature_importance_and_correlation().\n        metric_name (str): Name of the metric (e.g., \"accuracy\", \"loss\").\n\n    Returns:\n        plotly.graph_objects.Figure: Interactive 2x2 grid figure of bar plots.\n    \"\"\"\n    # Convert the results dict to a DataFrame for easier plotting\n    data = []\n    for param, stats in results.items():\n        data.append(\n            {\n                \"Parameter\": param,\n                \"Importance\": stats[\"importance\"],\n                \"Permutation Importance\": stats[\"permutation_importance\"],\n                \"Correlation (Pearson)\": stats[\"pearson\"],\n                \"Correlation (Spearman)\": stats[\"spearman\"],\n            }\n        )\n    df = pd.DataFrame(data)\n    df.sort_values(\"Importance\", ascending=False, inplace=True)\n\n    # Create a 2x2 subplot figure\n    fig = make_subplots(\n        rows=2,\n        cols=2,\n        subplot_titles=[\n            \"Parameter Importance\",\n            \"Pearson Correlation\",\n            \"Spearman Correlation\",\n            \"Permutation Importance\",\n        ],\n    )\n\n    # Define common properties\n    param_order = df[\"Parameter\"].tolist()\n\n    # 1. Importance plot (top-left)\n    fig.add_trace(\n        go.Bar(x=df[\"Importance\"], y=df[\"Parameter\"], orientation=\"h\", marker_color=\"royalblue\"), row=1, col=1\n    )\n\n    # 2. Pearson correlation plot (top-right)\n    fig.add_trace(\n        go.Bar(\n            x=df[\"Correlation (Pearson)\"],\n            y=df[\"Parameter\"],\n            orientation=\"h\",\n            marker_color=[\"seagreen\" if v &gt;= 0 else \"crimson\" for v in df[\"Correlation (Pearson)\"]],\n        ),\n        row=1,\n        col=2,\n    )\n\n    # 3. Spearman correlation plot (bottom-left)\n    fig.add_trace(\n        go.Bar(\n            x=df[\"Correlation (Spearman)\"],\n            y=df[\"Parameter\"],\n            orientation=\"h\",\n            marker_color=[\"seagreen\" if v &gt;= 0 else \"crimson\" for v in df[\"Correlation (Spearman)\"]],\n        ),\n        row=2,\n        col=1,\n    )\n\n    # 4. Permutation importance plot (bottom-right)\n    fig.add_trace(\n        go.Bar(x=df[\"Permutation Importance\"], y=df[\"Parameter\"], orientation=\"h\", marker_color=\"purple\"), row=2, col=2\n    )\n\n    # Update layout\n    fig.update_layout(\n        title=f\"Parameter Analysis for {metric_name}\",\n        height=max(600, 300 + 30 * len(df)),\n        width=1000,\n        showlegend=True,\n        legend={\"orientation\": \"h\", \"yanchor\": \"bottom\", \"y\": 1.02, \"xanchor\": \"right\", \"x\": 1},\n    )\n\n    # Update axes\n    fig.update_xaxes(title_text=\"Importance Score\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Pearson Correlation\", row=1, col=2)\n    fig.update_xaxes(title_text=\"Spearman Correlation\", row=2, col=1)\n    fig.update_xaxes(title_text=\"Permutation Importance\", row=2, col=2)\n\n    # Ensure consistent y-axis ordering across all subplots\n    for i in range(1, 3):\n        for j in range(1, 3):\n            fig.update_yaxes(categoryorder=\"array\", categoryarray=param_order, row=i, col=j)\n    fig.update_layout(showlegend=False)\n    return fig\n</code></pre>"},{"location":"api_references/#mlflow_sweep.plotting.plot_trial_timeline","title":"<code>plot_trial_timeline(df, start_col='start', end_col='end', run_col='run', status_col='status', color_map=None, title='Timeline Plot')</code>","text":"<p>Creates a Plotly timeline plot for trial runs.</p> <p>Parameters: - df: DataFrame containing trial data. - start_col: Name of the column containing start timestamps. - end_col: Name of the column containing end timestamps. - run_col: Name of the column identifying run IDs. - status_col: Name of the column specifying trial status. - color_map: Optional dict to specify colors for statuses. - title: Title of the plot.</p> Example <p>import pandas as pd data = { ...     \"start\": [\"2023-01-01 10:00:00\", \"2023-01-01 11:00:00\", \"2023-01-01 12:00:00\"], ...     \"end\": [\"2023-01-01 10:30:00\", \"2023-01-01 11:30:00\", \"2023-01-01 12:30:00\"], ...     \"run\": [\"Run 1\", \"Run 2\", \"Run 3\"], ...     \"status\": [\"finished\", \"failed\", \"pruned\"] ... } df = pd.DataFrame(data) fig = plot_trial_timeline(df)</p> Source code in <code>src/mlflow_sweep/plotting.py</code> <pre><code>def plot_trial_timeline(\n    df: pd.DataFrame,\n    start_col: str = \"start\",\n    end_col: str = \"end\",\n    run_col: str = \"run\",\n    status_col: str = \"status\",\n    color_map: dict | None = None,\n    title: str = \"Timeline Plot\",\n) -&gt; go.Figure:\n    \"\"\"\n    Creates a Plotly timeline plot for trial runs.\n\n    Parameters:\n    - df: DataFrame containing trial data.\n    - start_col: Name of the column containing start timestamps.\n    - end_col: Name of the column containing end timestamps.\n    - run_col: Name of the column identifying run IDs.\n    - status_col: Name of the column specifying trial status.\n    - color_map: Optional dict to specify colors for statuses.\n    - title: Title of the plot.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = {\n        ...     \"start\": [\"2023-01-01 10:00:00\", \"2023-01-01 11:00:00\", \"2023-01-01 12:00:00\"],\n        ...     \"end\": [\"2023-01-01 10:30:00\", \"2023-01-01 11:30:00\", \"2023-01-01 12:30:00\"],\n        ...     \"run\": [\"Run 1\", \"Run 2\", \"Run 3\"],\n        ...     \"status\": [\"finished\", \"failed\", \"pruned\"]\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; fig = plot_trial_timeline(df)\n\n    \"\"\"\n    # Default color map if none is provided\n    if color_map is None:\n        color_map = {\"finished\": \"blue\", \"failed\": \"red\", \"pruned\": \"orange\"}\n\n    # Ensure datetime columns are in proper format\n    df[start_col] = pd.to_datetime(df[start_col])\n    df[end_col] = pd.to_datetime(df[end_col])\n\n    # Ensure timeline data is at least 1 second long\n    zero_duration = df[start_col] == df[end_col]\n    df.loc[zero_duration, end_col] += pd.Timedelta(seconds=1)\n\n    # Create timeline plot\n    fig = px.timeline(df, x_start=start_col, x_end=end_col, y=run_col, color=status_col, color_discrete_map=color_map)\n\n    fig.update_layout(\n        title=title, xaxis_title=\"Datetime\", yaxis_title=\"Trial\", yaxis_autorange=\"reversed\", template=\"plotly_white\"\n    )\n\n    return fig\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sampler","title":"<code>mlflow_sweep.sampler</code>","text":""},{"location":"api_references/#mlflow_sweep.sampler.SweepSampler","title":"<code>SweepSampler</code>","text":"<p>Sampler for proposing new runs in a sweep based on the provided configuration and state.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SweepConfig</code> <p>The sweep configuration containing the command and parameters.</p> required <code>sweepstate</code> <code>SweepState</code> <p>Class managing the state of the sweep, including previous runs and metrics.</p> required Source code in <code>src/mlflow_sweep/sampler.py</code> <pre><code>class SweepSampler:\n    \"\"\"Sampler for proposing new runs in a sweep based on the provided configuration and state.\n\n    Args:\n        config (SweepConfig): The sweep configuration containing the command and parameters.\n        sweepstate (SweepState): Class managing the state of the sweep, including previous runs and metrics.\n    \"\"\"\n\n    def __init__(self, config: SweepConfig, sweepstate: SweepState) -&gt; None:\n        self.config = config\n        self.sweepstate = sweepstate\n\n    def propose_next(self) -&gt; tuple[str, dict] | None:\n        \"\"\"Propose the next run command and parameters based on the sweep configuration and state.\"\"\"\n        previous_runs = self.sweepstate.get_all(with_metric=self.config.metric.name if self.config.metric else \"\")\n        if len(previous_runs) &gt;= self.config.run_cap:\n            return None  # Stop proposing new runs if the cap is reached\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", category=ConvergenceWarning, message=\"The optimal value found for dimension 0 of parameter.*\"\n            )\n            sweep_config = sweep_module.next_run(sweep_config=self.config.model_dump(), runs=previous_runs)\n\n        if sweep_config is None:\n            return None  # Grid search is exhausted or no more runs can be proposed\n        proposed_parameters = {k: v[\"value\"] for k, v in sweep_config.config.items()}\n        command = self.replace_dollar_signs(self.config.command, proposed_parameters)\n        proposed_parameters[\"run\"] = len(previous_runs) + 1  # Increment run count for this sweep\n        proposed_parameters[\"sweep_run_id\"] = str(uuid.uuid4())  # Unique ID for this run\n        return command, proposed_parameters\n\n    @staticmethod\n    def replace_dollar_signs(string: str, parameters: dict) -&gt; str:\n        \"\"\"Replace ${parameter} with the actual parameter values.\"\"\"\n        for key, value in parameters.items():\n            string = sub(rf\"\\${{{key}}}\", str(value), string)\n        return string\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sampler.SweepSampler.propose_next","title":"<code>propose_next()</code>","text":"<p>Propose the next run command and parameters based on the sweep configuration and state.</p> Source code in <code>src/mlflow_sweep/sampler.py</code> <pre><code>def propose_next(self) -&gt; tuple[str, dict] | None:\n    \"\"\"Propose the next run command and parameters based on the sweep configuration and state.\"\"\"\n    previous_runs = self.sweepstate.get_all(with_metric=self.config.metric.name if self.config.metric else \"\")\n    if len(previous_runs) &gt;= self.config.run_cap:\n        return None  # Stop proposing new runs if the cap is reached\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", category=ConvergenceWarning, message=\"The optimal value found for dimension 0 of parameter.*\"\n        )\n        sweep_config = sweep_module.next_run(sweep_config=self.config.model_dump(), runs=previous_runs)\n\n    if sweep_config is None:\n        return None  # Grid search is exhausted or no more runs can be proposed\n    proposed_parameters = {k: v[\"value\"] for k, v in sweep_config.config.items()}\n    command = self.replace_dollar_signs(self.config.command, proposed_parameters)\n    proposed_parameters[\"run\"] = len(previous_runs) + 1  # Increment run count for this sweep\n    proposed_parameters[\"sweep_run_id\"] = str(uuid.uuid4())  # Unique ID for this run\n    return command, proposed_parameters\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sampler.SweepSampler.replace_dollar_signs","title":"<code>replace_dollar_signs(string, parameters)</code>  <code>staticmethod</code>","text":"<p>Replace ${parameter} with the actual parameter values.</p> Source code in <code>src/mlflow_sweep/sampler.py</code> <pre><code>@staticmethod\ndef replace_dollar_signs(string: str, parameters: dict) -&gt; str:\n    \"\"\"Replace ${parameter} with the actual parameter values.\"\"\"\n    for key, value in parameters.items():\n        string = sub(rf\"\\${{{key}}}\", str(value), string)\n    return string\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate","title":"<code>mlflow_sweep.sweepstate</code>","text":""},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState","title":"<code>SweepState</code>","text":"<p>Class to manage the state of a sweep in MLflow.</p> <p>The SweepState class provides methods to retrieve, save, and manage SweepRuns associated with a given sweep_id.</p> <p>Parameters:</p> Name Type Description Default <code>sweep_id</code> <code>str</code> <p>The ID of the sweep to manage.</p> required Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>class SweepState:\n    \"\"\"Class to manage the state of a sweep in MLflow.\n\n    The SweepState class provides methods to retrieve, save, and manage SweepRuns associated with a given sweep_id.\n\n    Args:\n        sweep_id: The ID of the sweep to manage.\n    \"\"\"\n\n    def __init__(self, sweep_id: str):\n        self.sweep_id = sweep_id\n        self.client = MlflowClient()\n\n    def get_all(self, with_metric: str = \"\") -&gt; list[ExtendedSweepRun]:\n        \"\"\"Retrieve all SweepRuns associated with the sweep_id.\"\"\"\n        mlflow_runs: list[Run] = mlflow.search_runs(  # ty: ignore[invalid-assignment]\n            search_all_experiments=True,\n            filter_string=f\"tag.mlflow.parentRunId = '{self.sweep_id}'\",\n            output_format=\"list\",\n        )\n\n        parameters = self.get_parameters()\n        if with_metric != \"\":\n            metric_history = []\n            for run in mlflow_runs:\n                history = MetricHistory(\n                    run_id=run.data.tags.get(\"mlflow.sweepRunId\"),\n                    metrics=[\n                        {with_metric: v.value} for v in self.client.get_metric_history(run.info.run_id, key=with_metric)\n                    ],\n                )\n                metric_history.append(history)\n\n            metric_history_sorted = sorted(metric_history, key=lambda x: x.run_id)\n\n        mlflow_runs_sorted = sorted(mlflow_runs, key=lambda run: run.data.tags.get(\"mlflow.sweepRunId\"))\n        parameters_sorted = sorted(parameters, key=lambda x: x[\"sweep_run_id\"])\n\n        if with_metric != \"\":\n            return [\n                self.convert_from_mlflow_runinfo_to_sweep_run(run, params, metrics)\n                for run, params, metrics in zip(mlflow_runs_sorted, parameters_sorted, metric_history_sorted)\n            ]\n        return [\n            self.convert_from_mlflow_runinfo_to_sweep_run(run, params)\n            for run, params in zip(mlflow_runs_sorted, parameters_sorted)\n        ]\n\n    def get(self, run_id: str) -&gt; ExtendedSweepRun:\n        \"\"\"Retrieve a SweepRun by its run_id.\n\n        Args:\n            run_id: The ID of the SweepRun to retrieve.\n\n        \"\"\"\n        mlflow_run: Run = mlflow.search_runs(  # ty: ignore[invalid-assignment]\n            search_all_experiments=True,\n            filter_string=f\"tag.mlflow.sweepRunId = '{run_id}'\",\n            output_format=\"list\",\n        )[0]\n        parameters = self.get_parameters()\n        parameters = next((p for p in parameters if p[\"sweep_run_id\"] == run_id), {})\n        return self.convert_from_mlflow_runinfo_to_sweep_run(mlflow_run, parameters)\n\n    def save(self, run_id: str):\n        \"\"\"Save the SweepRun to MLflow.\n\n        Args:\n            run_id: The ID of the SweepRun to save.\n\n        \"\"\"\n        sweep_run = self.get(run_id)\n        self.client.log_dict(\n            run_id=self.sweep_id,\n            dictionary=sweep_run.model_dump(),\n            artifact_file=f\"sweep_run_{sweep_run.id}.json\",\n        )\n\n    @staticmethod\n    def convert_from_mlflow_runinfo_to_sweep_run(\n        mlflow_run: Run, params: dict, metrics: None | MetricHistory = None\n    ) -&gt; ExtendedSweepRun:\n        \"\"\"Convert an MLflow Run to a SweepRun.\n\n        Args:\n            mlflow_run: The MLflow Run object to convert.\n            params: The parameters associated with the run.\n\n        Returns:\n            An ExtendedSweepRun object containing the run and parameter information.\n\n        \"\"\"\n        params = {k: {\"value\": v} for k, v in params.items() if k not in [\"run\", \"sweep_run_id\"]}\n        return ExtendedSweepRun(\n            id=mlflow_run.info.run_id,\n            name=mlflow_run.info.run_name,\n            summaryMetrics=mlflow_run.data.metrics,  # ty: ignore[unknown-argument]\n            history=[] if metrics is None else metrics.metrics,\n            config=params,\n            state=status_mapping(mlflow_run.info.status),\n            start_time=mlflow_run.info.start_time,\n            end_time=mlflow_run.info.end_time,\n        )\n\n    def get_parameters(self):\n        \"\"\"Retrieve the proposed parameters for previous runs.\"\"\"\n        if \"proposed_parameters.json\" not in [a.path for a in self.client.list_artifacts(self.sweep_id)]:\n            return []\n        artifact_uri = self.client.get_run(self.sweep_id).info.artifact_uri.replace(\"file://\", \"\")\n        table_path = Path(artifact_uri) / \"proposed_parameters.json\"\n        with Path.open(table_path) as file:\n            previous_runs: dict = json.load(file)\n        return [{previous_runs[\"columns\"][i]: row[i] for i in range(len(row))} for row in previous_runs[\"data\"]]\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState.convert_from_mlflow_runinfo_to_sweep_run","title":"<code>convert_from_mlflow_runinfo_to_sweep_run(mlflow_run, params, metrics=None)</code>  <code>staticmethod</code>","text":"<p>Convert an MLflow Run to a SweepRun.</p> <p>Parameters:</p> Name Type Description Default <code>mlflow_run</code> <code>Run</code> <p>The MLflow Run object to convert.</p> required <code>params</code> <code>dict</code> <p>The parameters associated with the run.</p> required <p>Returns:</p> Type Description <code>ExtendedSweepRun</code> <p>An ExtendedSweepRun object containing the run and parameter information.</p> Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>@staticmethod\ndef convert_from_mlflow_runinfo_to_sweep_run(\n    mlflow_run: Run, params: dict, metrics: None | MetricHistory = None\n) -&gt; ExtendedSweepRun:\n    \"\"\"Convert an MLflow Run to a SweepRun.\n\n    Args:\n        mlflow_run: The MLflow Run object to convert.\n        params: The parameters associated with the run.\n\n    Returns:\n        An ExtendedSweepRun object containing the run and parameter information.\n\n    \"\"\"\n    params = {k: {\"value\": v} for k, v in params.items() if k not in [\"run\", \"sweep_run_id\"]}\n    return ExtendedSweepRun(\n        id=mlflow_run.info.run_id,\n        name=mlflow_run.info.run_name,\n        summaryMetrics=mlflow_run.data.metrics,  # ty: ignore[unknown-argument]\n        history=[] if metrics is None else metrics.metrics,\n        config=params,\n        state=status_mapping(mlflow_run.info.status),\n        start_time=mlflow_run.info.start_time,\n        end_time=mlflow_run.info.end_time,\n    )\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState.get","title":"<code>get(run_id)</code>","text":"<p>Retrieve a SweepRun by its run_id.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The ID of the SweepRun to retrieve.</p> required Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>def get(self, run_id: str) -&gt; ExtendedSweepRun:\n    \"\"\"Retrieve a SweepRun by its run_id.\n\n    Args:\n        run_id: The ID of the SweepRun to retrieve.\n\n    \"\"\"\n    mlflow_run: Run = mlflow.search_runs(  # ty: ignore[invalid-assignment]\n        search_all_experiments=True,\n        filter_string=f\"tag.mlflow.sweepRunId = '{run_id}'\",\n        output_format=\"list\",\n    )[0]\n    parameters = self.get_parameters()\n    parameters = next((p for p in parameters if p[\"sweep_run_id\"] == run_id), {})\n    return self.convert_from_mlflow_runinfo_to_sweep_run(mlflow_run, parameters)\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState.get_all","title":"<code>get_all(with_metric='')</code>","text":"<p>Retrieve all SweepRuns associated with the sweep_id.</p> Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>def get_all(self, with_metric: str = \"\") -&gt; list[ExtendedSweepRun]:\n    \"\"\"Retrieve all SweepRuns associated with the sweep_id.\"\"\"\n    mlflow_runs: list[Run] = mlflow.search_runs(  # ty: ignore[invalid-assignment]\n        search_all_experiments=True,\n        filter_string=f\"tag.mlflow.parentRunId = '{self.sweep_id}'\",\n        output_format=\"list\",\n    )\n\n    parameters = self.get_parameters()\n    if with_metric != \"\":\n        metric_history = []\n        for run in mlflow_runs:\n            history = MetricHistory(\n                run_id=run.data.tags.get(\"mlflow.sweepRunId\"),\n                metrics=[\n                    {with_metric: v.value} for v in self.client.get_metric_history(run.info.run_id, key=with_metric)\n                ],\n            )\n            metric_history.append(history)\n\n        metric_history_sorted = sorted(metric_history, key=lambda x: x.run_id)\n\n    mlflow_runs_sorted = sorted(mlflow_runs, key=lambda run: run.data.tags.get(\"mlflow.sweepRunId\"))\n    parameters_sorted = sorted(parameters, key=lambda x: x[\"sweep_run_id\"])\n\n    if with_metric != \"\":\n        return [\n            self.convert_from_mlflow_runinfo_to_sweep_run(run, params, metrics)\n            for run, params, metrics in zip(mlflow_runs_sorted, parameters_sorted, metric_history_sorted)\n        ]\n    return [\n        self.convert_from_mlflow_runinfo_to_sweep_run(run, params)\n        for run, params in zip(mlflow_runs_sorted, parameters_sorted)\n    ]\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState.get_parameters","title":"<code>get_parameters()</code>","text":"<p>Retrieve the proposed parameters for previous runs.</p> Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>def get_parameters(self):\n    \"\"\"Retrieve the proposed parameters for previous runs.\"\"\"\n    if \"proposed_parameters.json\" not in [a.path for a in self.client.list_artifacts(self.sweep_id)]:\n        return []\n    artifact_uri = self.client.get_run(self.sweep_id).info.artifact_uri.replace(\"file://\", \"\")\n    table_path = Path(artifact_uri) / \"proposed_parameters.json\"\n    with Path.open(table_path) as file:\n        previous_runs: dict = json.load(file)\n    return [{previous_runs[\"columns\"][i]: row[i] for i in range(len(row))} for row in previous_runs[\"data\"]]\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.SweepState.save","title":"<code>save(run_id)</code>","text":"<p>Save the SweepRun to MLflow.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>The ID of the SweepRun to save.</p> required Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>def save(self, run_id: str):\n    \"\"\"Save the SweepRun to MLflow.\n\n    Args:\n        run_id: The ID of the SweepRun to save.\n\n    \"\"\"\n    sweep_run = self.get(run_id)\n    self.client.log_dict(\n        run_id=self.sweep_id,\n        dictionary=sweep_run.model_dump(),\n        artifact_file=f\"sweep_run_{sweep_run.id}.json\",\n    )\n</code></pre>"},{"location":"api_references/#mlflow_sweep.sweepstate.status_mapping","title":"<code>status_mapping(mlflow_status)</code>","text":"<p>Map MLflow run status to SweepRun state.</p> Source code in <code>src/mlflow_sweep/sweepstate.py</code> <pre><code>def status_mapping(mlflow_status: str) -&gt; RunState:\n    \"\"\"Map MLflow run status to SweepRun state.\"\"\"\n    if mlflow_status == \"RUNNING\":\n        return RunState.running\n    if mlflow_status == \"SCHEDULED\":\n        return RunState.pending\n    if mlflow_status == \"FINISHED\":\n        return RunState.finished\n    if mlflow_status == \"FAILED\":\n        return RunState.failed\n    return RunState.killed\n</code></pre>"},{"location":"api_references/#mlflow_sweep.utils","title":"<code>mlflow_sweep.utils</code>","text":""},{"location":"api_references/#mlflow_sweep.utils.calculate_feature_importance_and_correlation","title":"<code>calculate_feature_importance_and_correlation(metric_value, parameter_values)</code>","text":"<p>Calculate feature importance and correlation coefficients for hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>metric_value</code> <code>ndarray</code> <p>Array of metric values (e.g., validation loss).</p> required <code>parameter_values</code> <code>dict[str, ndarray]</code> <p>Dictionary where keys are parameter names and values are arrays of parameter values.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with parameter names as keys and dictionaries containing importance,   permutation importance, and correlation metrics as values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(42)  # For reproducibility\n&gt;&gt;&gt; metric = np.array([0.1, 0.2, 0.15, 0.25, 0.3])\n&gt;&gt;&gt; params = {\n...     'learning_rate': np.array([0.01, 0.02, 0.01, 0.03, 0.02]),\n...     'batch_size': np.array([32, 64, 32, 128, 64])\n... }\n&gt;&gt;&gt; result = calculate_feature_importance_and_correlation(metric, params)\n&gt;&gt;&gt; sorted(result.keys())\n['batch_size', 'learning_rate']\n&gt;&gt;&gt; all(k in result['learning_rate'] for k in ['importance', 'permutation_importance', 'pearson', 'spearman'])\nTrue\n</code></pre> Source code in <code>src/mlflow_sweep/utils.py</code> <pre><code>def calculate_feature_importance_and_correlation(\n    metric_value: np.ndarray, parameter_values: dict[str, np.ndarray]\n) -&gt; dict:\n    \"\"\"Calculate feature importance and correlation coefficients for hyperparameters.\n\n    Args:\n        metric_value (np.ndarray): Array of metric values (e.g., validation loss).\n        parameter_values (dict[str, np.ndarray]): Dictionary where keys are parameter names and values are arrays of\n            parameter values.\n\n    Returns:\n        dict: Dictionary with parameter names as keys and dictionaries containing importance,\n              permutation importance, and correlation metrics as values.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; np.random.seed(42)  # For reproducibility\n        &gt;&gt;&gt; metric = np.array([0.1, 0.2, 0.15, 0.25, 0.3])\n        &gt;&gt;&gt; params = {\n        ...     'learning_rate': np.array([0.01, 0.02, 0.01, 0.03, 0.02]),\n        ...     'batch_size': np.array([32, 64, 32, 128, 64])\n        ... }\n        &gt;&gt;&gt; result = calculate_feature_importance_and_correlation(metric, params)\n        &gt;&gt;&gt; sorted(result.keys())\n        ['batch_size', 'learning_rate']\n        &gt;&gt;&gt; all(k in result['learning_rate'] for k in ['importance', 'permutation_importance', 'pearson', 'spearman'])\n        True\n    \"\"\"\n    # Check for empty parameter set\n    if not parameter_values:\n        raise ValueError(\"Parameter values dictionary cannot be empty\")\n\n    # Check that all parameter arrays have the same length as the metric array\n    for param_name, param_array in parameter_values.items():\n        if len(param_array) != len(metric_value):\n            raise ValueError(\n                f\"Parameter '{param_name}' has {len(param_array)} values, but metric has {len(metric_value)} values\"\n            )\n\n    # Encode categorical parameters\n    encoded_params = {}\n    encoders = {}\n\n    for param_name, param_array in parameter_values.items():\n        if param_array.dtype.kind in {\"U\", \"S\", \"O\"}:  # Check for string or object type\n            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n            encoded = encoder.fit_transform(param_array.reshape(-1, 1))\n            encoders[param_name] = encoder\n            for i, category in enumerate(encoder.categories_[0]):\n                encoded_params[f\"{param_name}_{category}\"] = encoded[:, i]\n        else:\n            encoded_params[param_name] = param_array\n\n    data = np.column_stack(list(encoded_params.values()))\n\n    model = RandomForestRegressor()\n    model.fit(data, metric_value)\n    importances = model.feature_importances_\n\n    perm = permutation_importance(model, data, metric_value, n_repeats=30)\n    perm_importances = perm[\"importances_mean\"]\n\n    correlations = {}\n    for i, param in enumerate(parameter_values.keys()):\n        pearson_corr, _ = pearsonr(data[:, i], metric_value)\n        spearman_corr, _ = spearmanr(data[:, i], metric_value)\n        correlations[param] = {\"pearson\": pearson_corr, \"spearman\": spearman_corr}\n\n    return {\n        k: {\n            \"importance\": float(v),\n            \"permutation_importance\": float(perm_importances[i]),\n            \"pearson\": float(correlations[k][\"pearson\"]),\n            \"spearman\": float(correlations[k][\"spearman\"]),\n        }\n        for i, (k, v) in enumerate(zip(parameter_values.keys(), importances))\n    }\n</code></pre>"},{"location":"api_references/#mlflow_sweep.utils.current_time_convert","title":"<code>current_time_convert(ts_ms)</code>","text":"<p>Convert a timestamp in milliseconds to a formatted UTC string.</p> <p>Parameters:</p> Name Type Description Default <code>ts_ms</code> <code>int</code> <p>Timestamp in milliseconds.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted UTC time string in 'YYYY-MM-DD HH:MM:SS' format.</p> Source code in <code>src/mlflow_sweep/utils.py</code> <pre><code>def current_time_convert(ts_ms: int) -&gt; str:\n    \"\"\"Convert a timestamp in milliseconds to a formatted UTC string.\n\n    Args:\n        ts_ms (int): Timestamp in milliseconds.\n\n    Returns:\n        str: Formatted UTC time string in 'YYYY-MM-DD HH:MM:SS' format.\n    \"\"\"\n    dt_utc = datetime.datetime.utcfromtimestamp(ts_ms / 1000)\n    return dt_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"configuration/","title":"\ud83d\udcc4 Configuration","text":"<p>The core object that needs to be created to use this package is a config file writing in YAML format that contains the configuration for the hyperparameter optimization process. MLflow sweep utilizes this sweep library to sample hyperparameters. This package is developed by Weights and Biases and Mlflow sweeps configuration format is therefore very similar to the one used by Weights and Biases. This documentation part is therefore partly taken from here.</p> Difference to Weights and Biases <p>The main difference to the Weights and Biases sweep configuration is the following:</p> <ul> <li> <p>The <code>command</code> field is a single string where parameters are specified using the <code>${parameter_name}</code> syntax. In     Weights and Biases, the <code>command</code> field consist of a list of macros that determine how the command is run     and parameters are passed to the command.</p> </li> <li> <p>The <code>experiment_name</code> and <code>sweep_name</code> fields are used to create the experiment and sweep in MLflow. In Weights     and Biases, this more or less corresponds to the <code>project</code> and <code>name</code> fields in the sweep.</p> </li> <li> <p>Weights and Biases have a <code>entity</code> field for teams running sweeps, this is not present in MLflow sweeps.</p> </li> <li> <p>Weights and Biases have a <code>early_terminate</code> field to stop runs that are not performing well, this is not present     in MLflow sweeps (at the moment, will be added in the future).</p> </li> </ul> <p>A minimal configuration file looks like this:</p> sweep.yaml<pre><code>command:                      # Command to run the training script with parameters (required)\n  uv run example.py\n  --learning-rate ${learning_rate}\n  --batch-size ${batch_size}\nexperiment_name: sweep-demo   # Name of the experiment\nsweep_name: test-sweep        # Name of the sweep\nmethod: random                # Method for hyperparameter optimization\nmetric:                       # Metric to optimize\n  name: metric1\n  goal: maximize\nparameters:                   # Parameters to optimize (required)\n  learning_rate:\n    distribution: uniform\n    min: 0.001\n    max: 0.1\n  batch_size:\n    values: [16, 32, 64, 128]\nrun_cap: 10                   # Maximum number of runs to execute\n</code></pre> <p>The <code>experiment_name</code> corresponds to the name of the experiment where the sweep will be created. It corresponds directly to <code>mlflow.create_experiment</code> in the MLflow API. By default, this is set to the <code>Default</code> namespace. <code>sweep_name</code> is the name of the sweep that will be created in the experiment. It is used to group the runs of the sweep together and corresponds to the <code>mlflow.start_run</code> in the MLflow API. The <code>run_cap</code> is the maximum number of runs that will be executed in the sweep. If not specified, it will be set to 10.</p> <p>The remaining fields are explained in more details below.</p>"},{"location":"configuration/#command-configuration","title":"Command configuration","text":"<p>The <code>command</code> field is a string that contains the command to run the training script with the parameters that will be optimized. The parameters are specified using the <code>${parameter_name}</code> syntax, where <code>parameter_name</code> is the name of the parameter defined in the <code>parameters</code> section of the configuration file. Example of how to configure based on which package manager you are using</p> Standard Pythonpoetrypipenvuv <pre><code>command: python example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\n</code></pre> <pre><code>command: poetry run python example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\n</code></pre> <pre><code>command: pipenv run python example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\n</code></pre> <pre><code>command: uv run example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\n</code></pre> <p>In the same way, depending on how you pass parameters to your script you should adjust the command accordingly</p> Positional argumentsNamed argumentsHydra configurationNo hyphens <pre><code>command: uv run example.py ${learning_rate} ${batch_size}\n</code></pre> <pre><code>command: uv run example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\n</code></pre> <pre><code>command: uv run example.py --config-name config.yaml learning_rate=${learning_rate} batch_size=${batch_size}\n</code></pre> <pre><code>command: uv run example.py learning_rate=${learning_rate} batch_size=${batch_size}\n</code></pre>"},{"location":"configuration/#method-configuration","title":"Method configuration","text":"<p>Currently, MLflow sweep supports three methods for hyperparameter optimization: <code>bayes</code>, <code>random</code>, and <code>grid</code>. The <code>bayes</code> method uses Bayesian optimization to sample hyperparameters, which is a more efficient way to explore the hyperparameter space. The <code>random</code> method samples hyperparameters randomly from the specified distributions, while the <code>grid</code> method samples hyperparameters from a grid of values.</p>"},{"location":"configuration/#metric-configuration","title":"Metric configuration","text":"<p>The metric configuration is used to specify the metric that will be optimized during the sweep. This is only relevant if you are using the <code>bayes</code> method for hyperparameter optimization. The metric is specified as a dictionary with the following fields:</p> <pre><code>metric:                       # Metric to optimize\n  name: metric1\n  goal: maximize\n</code></pre> <p>where <code>goal</code> can be either <code>maximize</code> or <code>minimize</code>. The <code>name</code> field is the name of a metric which should be logged during the run using <code>mlflow.log_metric</code>.</p>"},{"location":"configuration/#parameter-configuration","title":"Parameter Configuration","text":"<p>The most complex part of the configuration file is the <code>parameters</code> section, which defines the hyperparameters to be optimized. It is a dictionary where each key is the name of a hyperparameter and then the options for that hyperparameter</p> sweep.yaml<pre><code>parameters:\n  parameter1:\n    options for parameter1\n  parameter2:\n    options for parameter2\n  ...\n</code></pre> <p>The following table lists the available options for each hyperparameter:</p> Search Constraint Description <code>values</code> Specifies all valid values for this hyperparameter. Compatible with grid. <code>value</code> Specifies the single valid value for this hyperparameter. Compatible with grid. <code>distribution</code> Specify a probability distribution. See the note following this table for information on default values. <code>probabilities</code> Specify the probability of selecting each element of values when using random. <code>min</code>, <code>max</code> <code>(int or float)</code> Maximum and minimum values. If int, for <code>int_uniform</code>-distributed hyperparameters. If float, for <code>uniform</code>-distributed hyperparameters. <code>mu</code> <code>(float)</code> Mean parameter for <code>normal</code>- or <code>lognormal</code>-distributed hyperparameters. <code>sigma</code> <code>(float)</code> Standard deviation parameter for <code>normal</code>- or <code>lognormal</code>-distributed hyperparameters. <code>q</code> <code>(float)</code> Quantization step size for quantized hyperparameters. <p>Info</p> <p>The following assumptions are made for the <code>distribution</code> key if not specified:</p> <ul> <li>Set to <code>categorical</code> if <code>values</code> is specified.</li> <li>Set to <code>constant</code> if <code>value</code> is specified.</li> <li>Set to <code>int_uniform</code> if <code>min</code> and <code>max</code> are specified and <code>min</code> and <code>max</code> are integers.</li> <li>Set to <code>uniform</code> if <code>min</code> and <code>max</code> are specified and <code>min</code> and <code>max</code> are floats.</li> </ul> Values for distribution key <p>The following table lists the available values for the <code>distribution</code> key in the <code>parameters</code> section.</p> Value for distribution key Description <code>constant</code> Constant distribution. Must specify the constant value (<code>value</code>) to use. <code>categorical</code> Categorical distribution. Must specify all valid values (<code>values</code>) for this hyperparameter. <code>int_uniform</code> Discrete uniform distribution on integers. Must specify <code>max</code> and <code>min</code> as integers. <code>uniform</code> Continuous uniform distribution. Must specify <code>max</code> and <code>min</code> as floats. <code>q_uniform</code> Quantized uniform distribution. Returns <code>round(X / q) * q</code> where <code>X</code> is uniform. <code>q</code> defaults to 1. <code>log_uniform</code> Log-uniform distribution. Returns a value <code>X</code> between <code>exp(min)</code> and <code>exp(max)</code> such that the natural logarithm is uniformly distributed between <code>min</code> and <code>max</code>. <code>log_uniform_values</code> Log-uniform distribution. Returns a value <code>X</code> between <code>min</code> and <code>max</code> such that <code>log(X)</code> is uniformly distributed between <code>log(min)</code> and <code>log(max)</code>. <code>q_log_uniform</code> Quantized log uniform. Returns <code>round(X / q) * q</code> where <code>X</code> is <code>log_uniform</code>. <code>q</code> defaults to 1. <code>q_log_uniform_values</code> Quantized log uniform. Returns <code>round(X / q) * q</code> where <code>X</code> is <code>log_uniform_values</code>. <code>q</code> defaults to 1. <code>inv_log_uniform</code> Inverse log uniform distribution. Returns <code>X</code>, where <code>log(1/X)</code> is uniformly distributed between <code>min</code> and <code>max</code>. <code>inv_log_uniform_values</code> Inverse log uniform distribution. Returns <code>X</code>, where <code>log(1/X)</code> is uniformly distributed between <code>log(1/max)</code> and <code>log(1/min)</code>. <code>normal</code> Normal distribution. Return value is normally distributed with mean <code>mu</code> (default 0) and standard deviation <code>sigma</code> (default 1). <code>q_normal</code> Quantized normal distribution. Returns <code>round(X / q) * q</code> where <code>X</code> is <code>normal</code>. <code>q</code> defaults to 1. <code>log_normal</code> Log normal distribution. Returns a value <code>X</code> such that the natural logarithm <code>log(X)</code> is normally distributed with mean <code>mu</code> (default 0) and <code>sigma</code> (default 1). <code>q_log_normal</code> Quantized log normal distribution. Returns <code>round(X / q) * q</code> where <code>X</code> is <code>log_normal</code>. <code>q</code> defaults to 1. <p>Here are examples of how to configure common hyperparameters:</p> Learning rateBatch sizeNumber of layersDropout rate <p>Learning rate is a common hyperparameter that is often optimized in machine learning models. Because values are on a logarithmic scale, we recommend using a log-uniform distribution to sample values.</p> sweep.yaml<pre><code>parameters:\n  learning_rate:\n    distribution: log_uniform\n    min: 1e-4\n    max: 1e-2\n</code></pre> <p>Batch size is usually set based on the available memory, but if you want to optimize it you may consider using a categorical distribution to sample values from a list of possible batch sizes.</p> sweep.yaml<pre><code>parameters:\n  batch_size:\n    distribution: categorical\n    values: [16, 32, 64, 128]\n</code></pre> <p>Number of layers is a common hyperparameter in deep learning models. It is usually an integer value, so you can use a discrete uniform distribution to sample values.</p> sweep.yaml<pre><code>parameters:\n  num_layers:\n    distribution: int_uniform\n    min: 1\n    max: 10\n</code></pre> <p>Dropout rate is a common hyperparameter in deep learning models. It is usually a float value between 0 and 1, so you can use a uniform distribution to sample values.</p> <pre><code>parameters:\n  dropout_rate:\n    distribution: uniform\n    min: 0.0\n    max: 0.5\n</code></pre>"},{"location":"configuration/#special-cases","title":"Special cases","text":"<ul> <li>If you have hyperparameters that are boolean values, most commonly the syntax for providing these as arguments would   be <code>--hyperparameter</code> or <code>--no-hyperparameter</code>. In this case, you can use the <code>categorical</code> distribution with two   strings:</li> </ul> sweep.yaml<pre><code>command:\n  uv run example.py ${hyperparameter}\nparameters:\n  hyperparameter:\n    distribution: categorical\n    values: [\"--hyperparameter\", \"--no-hyperparameter\"]\n</code></pre> <ul> <li>If you have hyperparameters which is loaded into your script as environment variables, you can just extend the   <code>command</code> field to first set the environment variables and then run the script:</li> </ul> sweep.yaml<pre><code>command: |\n  export HYPERPARAMETER=${hyperparameter} &amp;&amp;\n  uv run example.py --learning-rate ${learning_rate} --batch-size ${batch_size}\nparameters:\n  hyperparameter:\n    distribution: categorical\n    values: [\"value1\", \"value2\"]\n  learning_rate:\n    distribution: log_uniform\n    min: 1e-4\n    max: 1e-2\n  batch_size:\n    distribution: categorical\n    values: [16, 32, 64, 128]\n</code></pre> <ul> <li>If you have hyperparameters that are not passed as command line arguments but are instead loaded form a configuration   file you need to include custom logic before running the script. As an example, if you are storing hyperparameters   in a JSON file, you can use the <code>jq</code> command to modify the file before running the script:</li> </ul> config.json<pre><code>{\n  \"learning_rate\": 0.001,\n  \"batch_size\": 32,\n}\n</code></pre> sweep.yaml<pre><code>command: |\n  jq '.learning_rate = ${learning_rate} | .batch_size = ${batch_size}'\n  config.json &gt; config_updated.json &amp;&amp;\n  uv run example.py --config config_updated.json\nparameters:\n  learning_rate:\n    distribution: log_uniform\n    min: 1e-4\n    max: 1e-2\n  batch_size:\n    distribution: categorical\n    values: [16, 32, 64, 128]\n</code></pre>"},{"location":"examples/","title":"\ud83e\uddea Examples","text":"<p>Below are some examples of how to the main script for running a MLflow experiment could look like and what the corresponding sweep configuration file could look like.</p>"},{"location":"examples/#dummy-example","title":"Dummy example","text":"main.py main.py<pre><code>import random\nimport time\n\nimport mlflow\nimport typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef main(learning_rate: float = 0.001, batch_size: int = 32, dropout: bool = False):\n    \"\"\"\n    Example command that takes learning rate and batch size as arguments.\n    \"\"\"\n    typer.echo(f\"Learning Rate: {learning_rate}\")\n    typer.echo(f\"Batch Size: {batch_size}\")\n\n    with mlflow.start_run():\n        for i in range(random.randint(1, 5)):\n            mlflow.log_metric(\"metric1\", random.uniform(0, 1), step=i)\n            mlflow.log_metric(\"metric2\", random.uniform(0, 1), step=i)\n            time.sleep(random.uniform(0.1, 1))\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> sweep.yaml sweep.yaml<pre><code>command:\n  uv run main.py\n  --learning-rate ${learning_rate}\n  --batch-size ${batch_size}\nexperiment_name: sweep-demo\nsweep_name: test-sweep\nmethod: bayes\nmetric:\n  name: metric1\n  goal: maximize\nparameters:\n  learning_rate:\n    distribution: log_uniform\n    min: 0.001\n    max: 0.1\n  batch_size:\n    values: [16, 32, 64, 128]\nrun_cap: 10\n</code></pre>"},{"location":"examples/#scikit-learn-example","title":"Scikit-learn example","text":"main.py main.py<pre><code>import mlflow\nimport typer\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\napp = typer.Typer(help=\"Train a RandomForestClassifier on the breast cancer dataset\")\n\n\ndef none_str_to_none(value: str) -&gt; None | str:\n    \"\"\"Convert 'None' string to None.\"\"\"\n    return None if value.lower() == \"none\" else value\n\n\n@app.command()\ndef train(\n    n_estimators: int = typer.Option(100, help=\"Number of trees\"),\n    criterion: str = typer.Option(\"gini\", help=\"Criterion for splitting\"),\n    max_depth: int | None = typer.Option(None, help=\"Max depth of the trees\"),\n    min_samples_split: int = typer.Option(2, help=\"Min samples required to split a node\"),\n    min_samples_leaf: int = typer.Option(1, help=\"Min samples required at a leaf node\"),\n    max_features: str | None = typer.Option(\n        None, help=\"Number of features to consider for best split\", parser=none_str_to_none\n    ),\n    bootstrap: bool = typer.Option(True, help=\"Use bootstrap samples\"),\n):\n    \"\"\"Example command to train a RandomForestClassifier on the breast cancer dataset.\"\"\"\n    # Load dataset\n    data = datasets.load_breast_cancer()\n    x, y = data.data, data.target\n\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        criterion=criterion,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_features=max_features,\n        bootstrap=bootstrap,\n        random_state=42,\n    )\n\n    scores = cross_val_score(model, x, y, cv=5, scoring=\"accuracy\")\n    typer.echo(f\"Cross-validation scores: {scores}\")\n\n    with mlflow.start_run(run_name=\"random_forest\"):\n        mlflow.set_tag(\"model\", \"RandomForestClassifier\")\n        mlflow.log_param(\"n_estimators\", n_estimators)\n        mlflow.log_param(\"max_depth\", max_depth)\n        mlflow.log_param(\"min_samples_split\", min_samples_split)\n        mlflow.log_param(\"min_samples_leaf\", min_samples_leaf)\n        mlflow.log_param(\"max_features\", max_features)\n        mlflow.log_param(\"bootstrap\", bootstrap)\n        mlflow.log_metric(\"mean_accuracy\", scores.mean())\n        mlflow.log_metric(\"std_accuracy\", scores.std())\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> sweep.yaml sweep.yaml<pre><code>command:\n  uv run main.py\n  --n-estimators ${n_estimators}\n  --criterion ${criterion}\n  --max-depth ${max_depth}\n  --min-samples-split ${min_samples_split}\n  --min-samples-leaf ${min_samples_leaf}\n  --max-features ${max_features}\n  ${bootstrap}\nexperiment_name: example-sweep\nsweep_name: sklearn-sweep\nmethod: bayes\nmetric:\n  name: mean_accuracy\n  goal: maximize\nparameters:\n  n_estimators:\n    min: 50\n    max: 300\n    distribution: int_uniform\n  criterion:\n    values: [\"gini\", \"entropy\", \"log_loss\"]\n  max_depth:\n    min: 3\n    max: 20\n    distribution: int_uniform\n  min_samples_split:\n    values: [2, 4, 6, 8]\n  min_samples_leaf:\n    values: [1, 2, 4]\n  max_features:\n    values: [\"sqrt\", \"log2\", null]\n  bootstrap:\n    values: [\"--bootstrap\", \"--no-bootstrap\"]\nrun_cap: 50\n</code></pre>"},{"location":"examples/#mlflow-project-example","title":"MLflow project example","text":"main.py main.py<pre><code>import argparse\n\nimport mlflow\nimport mlflow.pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 10))\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef train_model(learning_rate, batch_size):\n    print(f\"Training with learning_rate={learning_rate}, batch_size={batch_size}\")\n\n    # Start MLflow run\n    with mlflow.start_run():\n        mlflow.log_param(\"learning_rate\", learning_rate)\n        mlflow.log_param(\"batch_size\", batch_size)\n\n        # Prepare dataset\n        transform = transforms.ToTensor()\n        train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n        # Model, loss, optimizer\n        model = SimpleNN()\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n        # Training loop\n        model.train()\n        for epoch in range(5):\n            total_loss = 0\n            for images, labels in train_loader:\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            avg_loss = total_loss / len(train_loader)\n            mlflow.log_metric(\"loss\", avg_loss, step=epoch)\n            print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n\n        # Save model\n        model_path = \"mnist_model.pth\"\n        torch.save(model.state_dict(), model_path)\n        print(f\"Model saved to {model_path}\")\n\n        # Log model to MLflow\n        mlflow.pytorch.log_model(model, \"model\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Train an MLP on MNIST with MLflow tracking\")\n\n    parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"Learning rate for optimizer\")\n    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training\")\n\n    args = parser.parse_args()\n\n    train_model(args.learning_rate, args.batch_size)\n</code></pre> sweep.yaml sweep.yaml<pre><code>command:\n  uv run mlflow run . -e train\n  -P learning_rate=${learning_rate}\n  -P batch_size=${batch_size}\nexperiment_name: sweep-demo\nsweep_name: test-sweep\nmethod: bayes\nmetric:\n  name: metric1\n  goal: maximize\nparameters:\n  learning_rate:\n    distribution: log_uniform\n    min: 0.001\n    max: 0.1\n  batch_size:\n    values: [16, 32, 64, 128]\nrun_cap: 10\n</code></pre> MLproject MLproject<pre><code>name: Sweep Project\n\nconda_env: conda_env.yaml\n\nentry_points:\n  train:\n    parameters:\n      learning_rate: {type: float, default: 0.01}\n      batch_size: {type: int, default: 32}\n    command: \"python main.py --learning_rate {learning_rate} --batch_size {batch_size}\"\n</code></pre>"},{"location":"quickstart/","title":"\ud83d\ude80 Getting Started","text":"<p>Start by creating a sweep configuration file (e.g., <code>sweep_config.yaml</code>) that defines what command to run, the parameters to sweep over and the sweep strategy. Here is an example configuration:</p> sweep_config.yaml<pre><code>command:\n  uv run example.py\n  --learning-rate ${learning_rate}\n  --batch-size ${batch_size}\nexperiment_name: sweep-demo\nsweep_name: test-sweep\nmethod: random\nparameters:\n  learning_rate:\n    distribution: uniform\n    min: 0.001\n    max: 0.1\n  batch_size:\n    values: [16, 32, 64, 128]\nrun_cap: 10\n</code></pre> <p>You can read more on this page about the configuration file and the available options.</p>"},{"location":"quickstart/#sweep-init","title":"Sweep init","text":"<p>After having created the sweep configuration file, you can initialize a sweep using the <code>mlflow sweep init</code> command. The command takes the path to the sweep configuration file as an argument:</p> <pre><code>mlflow sweep init sweep_config.yaml\n</code></pre> <p>You should see an output similar to this:</p> <pre><code>\u276f mlflow sweep init sweep_config.yaml\nInitializing sweep with configuration:\nSweepConfig(\n    command='uv run example.py --learning-rate ${learning_rate} --batch-size ${batch_size}',\n    experiment_name='sweep-demo',\n    sweep_name='test-sweep',\n    method=&lt;SweepMethodEnum.random: 'random'&gt;,\n    metric=MetricConfig(name='metric1', goal=&lt;GoalEnum.maximize: 'maximize'&gt;),\n    parameters={'learning_rate': {'distribution': 'uniform', 'min': 0.001, 'max': 0.1}, 'batch_size': {'values': [16, 32, 64, 128]}},\n    run_cap=10\n)\n2025/06/19 11:45:04 INFO mlflow.tracking.fluent: Experiment with name 'sweep-demo' does not exist. Creating a new experiment.\nSweep initialized with ID: 7556efd6d1fd46f0b3d893000e6f287a\n</code></pre> <p>This will create a sweep (a parent run in MLflow) which is just a MLflow run with the sweep configuration saved as an artifact. The last line of the output is the ID of the sweep run, which you will need for the next steps. If you try to spin op the MLflow UI, you will see the sweep run listed there under the experiment you specified in the configuration file</p> After running the init command a single mlflow is created."},{"location":"quickstart/#sweep-run","title":"Sweep run","text":"<p>Then you can use the <code>mlflow sweep run</code> command to start the sweep:</p> <pre><code>mlflow sweep run --sweep-id=&lt;sweep_id&gt;\n</code></pre> <p>The <code>--sweep-id</code> argument is the ID of the sweep run created in the previous step. It is an optional argument and if not provided we will look for the most recent initialized sweep run in the current directory. The <code>mlflow sweep run</code> command can be executed in parallel to parallelize the search process. The process will either stop when the <code>run_cap</code> is reached or when all combinations of the parameters have been tried (only applicable for grid search).</p> Example parallel execution. Each terminal is executing the run command and will report back to the main   sweep run to synchronize sampled hyperparameters and metrics. <p>If you try to spin op the MLflow UI, you should now see multiple child runs under the sweep run, each representing a single run of the command with a different set of hyperparameters. Each child run will contain all parameters, metrics, artifacts etc. that you are normally logging in your MLflow runs.</p> Runs are automatically nested under the sweep run for easy overview."},{"location":"quickstart/#sweep-finalize","title":"Sweep finalize","text":"<p>Finally, you can use the <code>mlflow sweep finalize</code> command to finalize the sweep:</p> <pre><code>mlflow sweep finalize --sweep-id=&lt;sweep_id&gt;\n</code></pre> <p>You should see an output similar to this:</p> <pre><code>\u276f uvr mlflow sweep finalize\n              Feature Importance and Correlation for metric1\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Parameter     \u2503 Importance \u2503 Permutation Importance \u2503 Pearson \u2503 Spearman \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 learning_rate \u2502     0.7913 \u2502                 0.9954 \u2502  0.1822 \u2502   0.1636 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 batch_size    \u2502     0.2087 \u2502                 0.1476 \u2502  0.1912 \u2502   0.1729 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This is an analysis of the parameter importance, permutation importance and correlation of the parameters with the metric you specified in the sweep configuration file. These results are visualized and these visualizations are saved as artifacts to the parent sweep run for future reference.</p>"}]}